"""
Policy Search API Server v0.14
==============================
FastAPI åç«¯ï¼Œä½¿ç”¨ Orchestrator æ™ºèƒ½æœç´¢ï¼š
  AI åˆ†æä¼ä¸šä¿¡æ¯ â†’ æ‹†åˆ†å¤šå±‚æœç´¢ä»»åŠ¡ â†’ Web Search â†’ AI è¯„å®¡å›è·¯ â†’ å»é‡æ±‡æ€»

é€šè¿‡ SSE å®æ—¶æ¨é€æœç´¢è¿‡ç¨‹å’Œç»“æœã€‚

å¯åŠ¨æ–¹å¼ï¼ˆé˜¿é‡Œäº‘æœåŠ¡å™¨ï¼‰ï¼š
    cd /opt/browser-sdk
    xvfb-run --auto-servernum python3 -u server.py

ç«¯å£: 8000ï¼ˆnginx åä»£ /api/ â†’ 8000ï¼‰
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from pathlib import Path

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from dotenv import load_dotenv

load_dotenv()
load_dotenv(".env.web_search")

# ç»Ÿä¸€æ¨¡å‹
from models import WorkerResult

# Orchestrator æ™ºèƒ½è°ƒåº¦
from orchestrator import Orchestrator


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æœç´¢æ—¥å¿—æŒä¹…åŒ– â€” æ¯æ¬¡æœç´¢ä¿å­˜å®Œæ•´æ—¥å¿—æ–‡ä»¶
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SEARCH_LOG_DIR = Path("/opt/browser-sdk/search_logs")
SEARCH_LOG_DIR.mkdir(parents=True, exist_ok=True)


def save_search_log(mode: str, query: str, log_lines: list[str], result: "WorkerResult"):
    """
    ä¿å­˜ä¸€æ¬¡æœç´¢çš„å®Œæ•´æ—¥å¿—åˆ°æ–‡ä»¶ã€‚
    æ–‡ä»¶å: search_YYYYMMDD_HHMMSS_{mode}.log
    å†…å®¹: æœç´¢å‚æ•° â†’ è¿‡ç¨‹æ—¥å¿— â†’ ç»“æœæ‘˜è¦ï¼ˆå«æ¸…æ™°çš„é“¾æ¥å’ŒPDFé“¾æ¥ï¼‰
    """
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = SEARCH_LOG_DIR / f"search_{ts}_{mode}.log"

    lines = []
    lines.append("=" * 70)
    lines.append(f"æœç´¢æ—¥å¿— â€” {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"æ¨¡å¼: {mode}")
    lines.append(f"æŸ¥è¯¢: {query}")
    lines.append(f"Worker: {result.worker}")
    lines.append(f"è€—æ—¶: {result.duration}s")
    lines.append(f"çŠ¶æ€: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
    if result.error:
        lines.append(f"é”™è¯¯: {result.error}")
    lines.append("=" * 70)

    # è¿‡ç¨‹æ—¥å¿—
    lines.append("")
    lines.append("â”€â”€ æœç´¢è¿‡ç¨‹ â”€â”€")
    for log_line in log_lines:
        lines.append(log_line)

    # Token ç”¨é‡
    if result.token_usage:
        lines.append("")
        lines.append(f"â”€â”€ Token ç”¨é‡ â”€â”€")
        lines.append(json.dumps(result.token_usage, ensure_ascii=False, indent=2))

    # å¼•ç”¨æ¥æº
    if result.sources:
        lines.append("")
        lines.append(f"â”€â”€ å¼•ç”¨æ¥æº ({len(result.sources)} ä¸ª) â”€â”€")
        for i, url in enumerate(result.sources, 1):
            lines.append(f"  {i}. {url}")

    # æ”¿ç­–ç»“æœ â€” æ¸…æ™°åˆ—å‡ºæ¯æ¡æ”¿ç­–çš„é“¾æ¥å’ŒPDF
    lines.append("")
    lines.append(f"â”€â”€ æœç´¢ç»“æœ: {result.policy_count} æ¡æ”¿ç­– â”€â”€")
    if result.policies:
        for i, p in enumerate(result.policies, 1):
            lines.append("")
            lines.append(f"  [{i}] {p.title}")
            lines.append(f"       æ¥æº: {p.source or 'æœªçŸ¥'}")
            lines.append(f"       æ—¥æœŸ: {p.date or 'æœªçŸ¥'}")
            lines.append(f"       è¡Œä¸š: {p.industry or 'æœªçŸ¥'}")
            lines.append(f"       æ‘˜è¦: {p.summary or 'æ— '}")
            lines.append(f"       æ‰¶æŒ: {p.support or 'æ— '}")
            lines.append(f"       ğŸ”— åŸæ–‡é“¾æ¥: {p.url or 'æ— '}")
            lines.append(f"       ğŸ“¥ PDFé“¾æ¥:  {p.pdf_url or 'æ— '}")
    else:
        lines.append("  (æ— ç»“æœ)")

    # LLM åŸå§‹å›ç­”
    if result.raw_answer:
        lines.append("")
        lines.append("â”€â”€ LLM åŸå§‹å›ç­” â”€â”€")
        lines.append(result.raw_answer[:5000])

    lines.append("")
    lines.append("=" * 70)

    try:
        filename.write_text("\n".join(lines), encoding="utf-8")
        logging.getLogger(__name__).info(f"æœç´¢æ—¥å¿—å·²ä¿å­˜: {filename}")
    except Exception as e:
        logging.getLogger(__name__).error(f"ä¿å­˜æœç´¢æ—¥å¿—å¤±è´¥: {e}")

    return str(filename)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥å¿—æ•è·å™¨ â€” æŠŠ browser-use æ—¥å¿—æ¨é€åˆ° SSE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LogCapture(logging.Handler):
    """æ•è·æ—¥å¿—åˆ° asyncio.Queueï¼Œä¾› SSE æµå¼æ¨é€"""

    # å¿½ç•¥çš„ logger åç§°å‰ç¼€ï¼ˆå¤ªåµæˆ–æ— å…³ï¼‰
    _IGNORE = {'uvicorn', 'httpx', 'httpcore', 'asyncio', 'watchfiles',
               'multipart', 'hpack', 'h2', 'charset_normalizer', 'PIL'}

    def __init__(self):
        super().__init__()
        self.queue: asyncio.Queue = asyncio.Queue()
        self._loop = None

    def set_loop(self, loop):
        self._loop = loop

    def emit(self, record):
        # æ’é™¤ç³»ç»Ÿ/ç½‘ç»œåº“çš„å™ªéŸ³æ—¥å¿—
        top = record.name.split('.')[0]
        if top in self._IGNORE:
            return
        msg = self.format(record)
        if msg.strip() and self._loop and self._loop.is_running():
            try:
                self._loop.call_soon_threadsafe(self.queue.put_nowait, msg)
            except Exception:
                pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI App
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(title="Policy Search API", version="0.14")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/api/policy-search/stream")
async def policy_search_stream(
    company_name: str = Query("", description="ä¼ä¸šåç§°"),
    industry: str = Query(..., description="è¡Œä¸šï¼Œå¦‚ å…‰é€šä¿¡"),
    region: str = Query("ä¸Šæµ·", description="åœ°åŒº"),
    district: str = Query("", description="åŒºï¼Œå¦‚ æµ¦ä¸œæ–°åŒº"),
    tags: str = Query("", description="ä¼ä¸šæ ‡ç­¾ï¼Œé€—å·åˆ†éš”"),
    registered_capital: str = Query("", description="æ³¨å†Œèµ„æœ¬"),
    employees: str = Query("", description="å‘˜å·¥è§„æ¨¡"),
    founded: str = Query("", description="æˆç«‹æ—¶é—´"),
):
    """SSE æµå¼æ™ºèƒ½æœç´¢ â€” Orchestrator é©±åŠ¨"""

    # æ„å»ºä¼ä¸šä¿¡æ¯ dict
    company_info = {
        "name": company_name or f"{region}{district} {industry}ä¼ä¸š",
        "industry": industry,
        "region": f"{region} {district}".strip() if district else region,
        "tags": [t.strip() for t in tags.split(",") if t.strip()] if tags else [],
    }
    if registered_capital:
        company_info["registered_capital"] = registered_capital
    if employees:
        company_info["employees"] = employees
    if founded:
        company_info["founded"] = founded

    async def event_generator():
        log_lines = []
        log_queue: asyncio.Queue = asyncio.Queue()

        def on_log(msg: str):
            """orchestrator çš„æ—¥å¿—å›è°ƒ â†’ æ”¾å…¥é˜Ÿåˆ—"""
            try:
                log_queue.put_nowait(msg)
            except Exception:
                pass

        yield _sse({'type': 'status', 'message': 'ğŸ§  æ™ºèƒ½æœç´¢å¯åŠ¨ä¸­...'})

        orch = Orchestrator(
            on_log=on_log,
            time_budget=180.0,
            max_rounds=2,
            request_delay=3.0,
        )

        # åœ¨åå°çº¿ç¨‹è¿è¡Œ orchestrator
        result_holder = {}

        async def run_orch():
            result_holder['result'] = await orch.run(company_info, skip_browse_use=True)

        task = asyncio.ensure_future(run_orch())

        # è¾¹æ‰§è¡Œè¾¹æ¨é€æ—¥å¿—
        while not task.done():
            try:
                msg = await asyncio.wait_for(log_queue.get(), timeout=1.0)
                if msg.strip():
                    log_lines.append(msg.strip())
                    yield _sse({'type': 'log', 'message': msg.strip()})
            except asyncio.TimeoutError:
                yield _sse({'type': 'heartbeat'})

        # æ¨é€å‰©ä½™æ—¥å¿—
        while not log_queue.empty():
            msg = log_queue.get_nowait()
            if msg.strip():
                log_lines.append(msg.strip())
                yield _sse({'type': 'log', 'message': msg.strip()})

        # è·å–ç»“æœ
        result: WorkerResult = result_holder.get('result')
        if result is None:
            # task å¯èƒ½æŠ›å¼‚å¸¸
            try:
                task.result()  # è§¦å‘å¼‚å¸¸
            except Exception as e:
                yield _sse({'type': 'error', 'message': str(e)})
                return

        # ä¿å­˜å®Œæ•´æ—¥å¿—åˆ°æ–‡ä»¶
        query_label = f"{company_info['name']} ({industry} @ {company_info['region']})"
        log_file = save_search_log("smart", query_label, log_lines, result)
        yield _sse({'type': 'log', 'message': f'ğŸ’¾ æ—¥å¿—å·²ä¿å­˜: {log_file}'})

        yield _sse({'type': 'result', 'data': result.to_sse_result()})
        yield _sse({'type': 'done'})

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


def _sse(data: dict) -> str:
    """æ ¼å¼åŒ– SSE æ¶ˆæ¯"""
    return f"data: {json.dumps(data, ensure_ascii=False, default=str)}\n\n"


@app.get("/api/health")
async def health():
    return {"status": "ok", "version": "0.14", "time": datetime.now().isoformat()}


@app.get("/api/logs")
async def get_logs(n: int = Query(80, description="è¡Œæ•°")):
    """è¯»å–æœåŠ¡å™¨æ—¥å¿—ï¼ˆä¸ä¾èµ– SSHï¼‰"""
    import subprocess
    # è¯» server.log
    try:
        r1 = subprocess.run(["tail", f"-{n}", "/tmp/server.log"], capture_output=True, text=True, timeout=5)
        server_log = r1.stdout
    except Exception as e:
        server_log = f"Error: {e}"
    # è¯» browser_use_debug.log
    try:
        r2 = subprocess.run(["tail", f"-{n}", "/tmp/browser_use_debug.log"], capture_output=True, text=True, timeout=5)
        debug_log = r2.stdout
    except Exception as e:
        debug_log = f"Error: {e}"
    return {"server_log": server_log, "debug_log": debug_log}


@app.get("/api/search-logs")
async def list_search_logs():
    """åˆ—å‡ºæ‰€æœ‰æœç´¢æ—¥å¿—æ–‡ä»¶"""
    logs = []
    for f in sorted(SEARCH_LOG_DIR.glob("search_*.log"), reverse=True):
        logs.append({
            "filename": f.name,
            "size": f.stat().st_size,
            "modified": datetime.fromtimestamp(f.stat().st_mtime).isoformat(),
        })
    return {"logs": logs, "count": len(logs)}


@app.get("/api/search-logs/{filename}")
async def get_search_log(filename: str):
    """è¯»å–æŸä¸ªæœç´¢æ—¥å¿—çš„å®Œæ•´å†…å®¹"""
    filepath = SEARCH_LOG_DIR / filename
    if not filepath.exists() or not filepath.name.startswith("search_"):
        return JSONResponse(status_code=404, content={"error": "æ—¥å¿—ä¸å­˜åœ¨"})
    content = filepath.read_text(encoding="utf-8")
    return {"filename": filename, "content": content}


if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Policy Search API v0.14 (Orchestrator æ™ºèƒ½æœç´¢)")
    print("   http://0.0.0.0:8000")
    print("   æ™ºèƒ½æœç´¢: /api/policy-search/stream?industry=å…‰é€šä¿¡&region=ä¸Šæµ·")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
